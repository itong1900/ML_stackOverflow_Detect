---
title: "StackOverflow_AutoDetect"
author: "Yitong Chen"
date: "10/23/2020"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
library(tm)
library(MASS)
library(rpart)
library(boot)
library(SnowballC)
library(wordcloud)
library(caTools)
library(dplyr)
library(rpart.plot)
library(randomForest)
library(caret)
```

```{r}
## load the data, and a quick view of data, the data is consisted of Title, Body, score, we'll mainly dig in to Title and Body and infer score
data = read.csv("ggplot2questions2016_17.csv", sep = ",", stringsAsFactors = FALSE)
clean_data = data
head(data,2)
```

## 1. Data Cleaning <br/>

Let's start with Data Cleaning <br/>

i)   remove <html> tag, as it's not meaningful in prediction and add unecessary noise <br/>
ii)  remove $\n \t$   <br/>
iii) all text convert to lower case  <br/>
iv)  remove puntunctuation    <br/>
v)   remove stopwords(more details will be given at that step)   <br/>
vi)  stem the documents      <br/>

```{r}
## doing i, ii
clean_data$Body = gsub("<.*?>", "",clean_data$Body)
clean_data$Body = gsub("[\n\t]", "", clean_data$Body)
```

```{r}
## before going to next steps, convert all body to corpus for easier analysis
corpusBody = Corpus(VectorSource(clean_data$Body))#create corpus
```

```{r, message=FALSE, warning=FALSE}
## step iii to lower
corpusBody = tm_map(corpusBody, tolower)#to lower case
## step iv remove punctuation
corpusBody = tm_map(corpusBody, removePunctuation)#remove punctuation
```


From Wikipedia, Stop words are defined as ""In computing, stop words are words which are filtered out before or after processing of natural language data." In our case, we'll remove top 10 stop words in English, as well as "ggplot" and "ggplot2" since they are over common in this topic
```{r, warning=FALSE, message=FALSE}
## step v 
stopwords("english")[1:10] # find top stopwords
length(stopwords("english"))
corpusBody = tm_map(corpusBody, removeWords, c("ggplot2","ggplot", stopwords("english")))
```

Also from Wikipedia, word steming is defined as "In linguistics, a stem is a part of a word used with slightly different meanings and would depend on the morphology of the language in question. In Athabaskan linguistics, for example, a verb stem is a root that cannot appear on its own, and that carries the tone of the word". We'll apply the steming to the corpusBody as our last data cleaning process.
```{r, warning=FALSE, message=FALSE}
## step vi stem the documents
corpusBody = tm_map(corpusBody, stemDocument)
```

```{r}
## Finally, as an example, let's look at the how the body of our first row are finally converted 
strwrap(corpusBody[[1]])
```



## 2. data processing <br/>
Next, we are doing the following, <br/>
vii)  construct the feature matrix by looking at the sparsity and frequencies of the words. <br/>
viii) do the same procedures on titles <br/> 
ix)   combine the matrix of textbody and title <br/><br/>

To select the words for learning, we look at their sparsity and frequencies appearing in the text.
we finally select the words as features by looking at the sparsity and frequencies appearing in the text. After trying on
frequencies ranges from 90%~99%, we found selecting the words that are appearing in 92% of the text and appearing more than 1500 times in total works nicely with the models. There are a total of 95 words satisfying this criteria.<br/>
Therefore, A feature matrix with of 95 columns obtained from the body is constructed as dfBody
```{r}
## step vii
frequenciesBody = DocumentTermMatrix(corpusBody)
frequenciesBody

findFreqTerms(frequenciesBody, lowfreq=1500)
sparseBody = removeSparseTerms(frequenciesBody, 0.92)
sparseBody
```

```{r}
dfBody = as.data.frame(as.matrix(sparseBody))
colnames(dfBody) = make.names(colnames(dfBody))
```

do the same procedures on titles like what we did to the text body.
```{r, warning=FALSE}
## step viii
clean_data$Title =  gsub("<.*?>", "",clean_data$Title)
clean_data$Title =  gsub("[\n\t]", "", clean_data$Title)  
corpusTitle = Corpus(VectorSource(clean_data$Title))#create corpus

corpusTitle = tm_map(corpusTitle, tolower)#to lower case

corpusTitle = tm_map(corpusTitle, removePunctuation)#remove punctuation

#remove stopwords
corpusTitle = tm_map(corpusTitle, removeWords, c("ggplot2","ggplot", stopwords("english")))
#STEM the document
corpusTitle = tm_map(corpusTitle, stemDocument)
```

Similar to how we treat body, by looking at the words that frequently appear, I finally selected the words that appear in
more than 96% of the titles as the characteristics for Title. Another feature matrix with 20 columns is constructed from the Title corpus.
```{r}
## get sparsity and frequencies
frequenciesTitle = DocumentTermMatrix(corpusTitle)

findFreqTerms(frequenciesTitle, lowfreq=120)
findFreqTerms(frequenciesTitle, lowfreq=80)
sparseTitle = removeSparseTerms(frequenciesTitle, 0.96)

dfTitle = as.data.frame(as.matrix(sparseTitle))
colnames(dfTitle) = make.names(colnames(dfTitle))
```

Finally, we constructed the full feature matrix, by merging the feature matrix obtained from title corpus with the one
constructed by body corpus, with a total of 116 columns, 115 features from body and text, and 1 column for labeling the
usefulness(>=1 as useful, and <1 as not useful). 
```{r}
## step ix) construct the full matrix
## Usefulness of each stackoverflow
clean_data$useful = as.factor(as.numeric(clean_data$Score >= 1))
#################
feat_matrix <- cbind(dfTitle,dfBody) 
df_features <- as.data.frame(feat_matrix)
df_features$useful = clean_data$useful
colnames(df_features) <- make.names(colnames(df_features),unique=TRUE)
```

## 3.Model fitting, Cross Validation, Model Selections
We will do the following in this chapter, <br/>
x)   Train Test Split <br/>
xi)  Fit models, including Baseline Model, Random Forest, Logistic Regression, Stepwise Regression, LDA and CART. <br/> 
xii) Performance Analysis and Model selection <br/><br/>

70% train, and 30% test
```{r}
## step x
splt = sample.split(df_features$useful, SplitRatio = 0.7)
stackTrain = df_features %>% filter(splt == TRUE)
stackTest = df_features %>% filter(splt == FALSE)
```


#### 3a) Baseline Model
```{r}
print("based on train set distribution, we'll always predict 0")
table(stackTrain$useful)

confusion_m <- table(stackTest$useful)
confusion_m 
paste("Baseline Model Accuracy: ", confusion_m[[1]]/ sum(confusion_m))  
```

#### 3b) Basic random forest
After a few testing, I found using parameters ntree = 500, and mtry = 60 performs best among all, achieving accuracy, TPR, FPR as followed
```{r, warning=FALSE}
stackRF = randomForest(useful ~ ., data=stackTrain, mtry = 60, ntree = 500)  
PredictRF = predict(stackRF, newdata = stackTest) 
confusion_m <- table(stackTest$useful, PredictRF)
confusion_m

paste("Random Forest Accuracy: ", (confusion_m[1] + confusion_m[4])/sum(confusion_m)) 

paste("Random Forest TPR: ", confusion_m[4]/(confusion_m[4] + confusion_m[3]))

paste("Random Forest FPR: ", confusion_m[2]/(confusion_m[2] + confusion_m[1]))
```

#### 3c) Logistic regression
```{r}
stackLog = glm(useful ~ ., data=stackTrain, family = "binomial")
PredictLog = predict(stackLog, newdata = stackTest, type = "response")
confusion_m <- table(stackTest$useful, PredictLog > 0.5)
confusion_m 

paste("Logistic Regression Accuracy: ", (confusion_m[1] + confusion_m[4])/sum(confusion_m)) 

paste("Logistic Regression TPR: ", confusion_m[4]/(confusion_m[4] + confusion_m[3]))

paste("Logistic Regression FPR: ", confusion_m[2]/(confusion_m[2] + confusion_m[1]))
```

#### 3d) stepwise regression
since the actual run time is too long to knit as an md file, I comment out the code put down the result ran previously, 
```{r}
#stackStepLog = step(stackLog, direction = "backward")
#summary(queStepLog)
#length(stackStepLog$coefficients)
#PredStepLog = predict(stackStepLog, newdata = stackTest, type = "response")

#confusion_m <- table(stackTest$useful, PredStepLog > 0.5)
#confusion_m

paste("Stepwise Logistic Regression Accuracy: ", (669+529)/sum(669,468,574,529)) 

paste("Stepwise Logistic TPR: ", 529/(529 + 468))

paste("Stepwise Logistic FPR: ", 574/(574 + 669))
```

#### 3e) LDA
```{r}
stackLDA = lda(useful ~ ., data=stackTrain)

PredLDA = predict(stackLDA, newdata = stackTest)$class
confusion_m <- table(stackTest$useful, PredLDA)
confusion_m

paste("LDA Accuracy: ", (confusion_m[1] + confusion_m[4])/sum(confusion_m)) 

paste("LDA TPR: ", confusion_m[4]/(confusion_m[4] + confusion_m[3]))

paste("LDA FPR: ", confusion_m[2]/(confusion_m[2] + confusion_m[1]))
```


#### 3f) CART
```{r, warning=FALSE}
library(e1071)

## train cart
stackCart = train(useful ~ . , data = stackTrain, method = "rpart", tuneGrid = data.frame(cp=seq(0,0.4,0.001)), trControl = trainControl(method = "cv", number = 5))

stackCART = stackCart$finalModel
prp(stackCART)
PredCART <- predict(stackCART, newdata = stackTest, type = "class")
ggplot(stackCart$results, aes(x = cp, y = Accuracy)) + geom_point(size = 2) + geom_line() + ylab("CV Accuracy") + theme_bw() + theme(axis.title = element_text(size = 18), axis.text = element_text(size = 18))

confusion_m <- table(stackTest$useful, PredCART)
confusion_m

paste("CART Accuracy: ", (confusion_m[1] + confusion_m[4])/sum(confusion_m)) 

paste("CART TPR: ", confusion_m[4]/(confusion_m[4] + confusion_m[3]))

paste("CART FPR: ", confusion_m[2]/(confusion_m[2] + confusion_m[1]))
```

## 4.Summary







